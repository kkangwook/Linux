* 준비 : master와 slave1 서버 ON

* 3. Hadoop/Yarn/Historyserver 시작 : Master에서 작업 

1) hadoop/yarn 데몬 시작 
[hadoop@master ~]$ start-all.sh

Historyserver 데몬 시작(mapReduce 작업을 위해서 필요) 
[hadoop@master ~]$ mr-jobhistory-daemon.sh start historyserver

2) 각 서버 데몬 실행 확인 
- NameNode 동작여부 확인
[hadoop@master ~]$ jps   

- DataNode의 동작여부 확인
[hadoop@slave1 ~]$ jps


* 5. MapReduce word Count 실습
1) word count 파일 준비 
[hadoop@master ~]$ hdfs dfs -mkdir /test
[hadoop@master ~]$ 
[hadoop@master ~]$ hdfs dfs -put ./hadoop-3.3.6/NOTICE.txt /test
[hadoop@master ~]$ 
[hadoop@master ~]$ hdfs dfs -cat /test/NOTICE.txt

# 2) word Count 실행 
[hadoop@master ~]$ hadoop jar $HADOOP_HOME/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.3.6.jar wordcount /test/NOTICE.txt  /output


* 4. 워드 카운트 결과 보기 
[hadoop@master ~]$ hdfs dfs -ls /output
[hadoop@master ~]$ hdfs dfs -cat /output/part-r-00000

http://localhost:8088/

* 5. HDFS 명령어 실습 : CSV 다운로드 & 업로드  
# 1) csv 파일 다운로드 
~]$ wget http://vincentarelbundock.github.io/Rdatasets/datasets.csv
~]$ cat datasets.csv

* 2) hdfs에 csv 파일 올리기 ls
[hadoop@master ~]$ hdfs dfs -mkdir /data  # hdfs 디렉터리 만들기 
[hadoop@master ~]$ hdfs dfs -put datasets.csv /data  # 로컬 파일 hdfs 디렉터리에 파일 올리기 
[hadoop@master ~]$ hdfs dfs -ls /data  # - hdfs 디렉터리 보기 
[hadoop@master ~]$ hdfs dfs -cat /data/datasets.csv  #- hdfs 파일 내용 보기
[hadoop@master ~]$ hdfs dfs -cp /data/datasets.csv  /data/datasets2.csv  # - hdfs 파일 이동
[hadoop@master ~]$ hdfs dfs -ls /data  # - hdfs 디렉터리 보기 

*3) hdfs에 파일 내리기 
[hadoop@master ~]$ mkdir hdfs_file  # 사용자 디렉터리 하위에 hdfs_file 디렉터리 생성 
[hadoop@master ~]$ hdfs dfs -get /data/datasets.csv  hdfs_file  #- hdfs 파일 로컬 복사

* 6. Hadoop/Yarn/Historyserver 종료
[hadoop@master ~]$ stop-all.sh
[hadoop@master ~]$ mr-jobhistory-daemon.sh stop historyserver






